import pandas as pd
import numpy as np
import os
import pickle
import re

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier

from gensim.models import Word2Vec


# ---------------- CLEAN TEXT ----------------
def clean_text(text):
    text = str(text).lower()

    # üî• handle negations
    text = text.replace("not good", "bad")
    text = text.replace("not great", "bad")
    text = text.replace("not effective", "ineffective")
    text = text.replace("not working", "failed")

    # üî• real-world replacements
    replacements = {
        "inflation": "price_increase",
        "unemployment": "job_loss",
        "jobless": "job_loss",
        "jobs are hard to find": "job_loss",
        "expensive": "high_cost",
        "price rise": "price_increase",
        "petrol price": "high_cost",
        "frustrated": "negative_feeling",
        "struggling": "negative_feeling"
    }

    for k, v in replacements.items():
        text = text.replace(k, v)

    # remove special chars
    text = re.sub(r"[^a-z\s]", "", text)

    return text.strip()


# ---------------- LOAD DATA ----------------
df = pd.read_csv("data/dataset.csv")
df.columns = df.columns.str.strip()

print("üìä Dataset Preview:")
print(df.head())


# ---------------- CLEAN DATA ----------------
df["text"] = df["text"].astype(str).apply(clean_text)

df = df[df["text"].str.strip() != ""]
df = df.drop_duplicates(subset="text")

print("\nüìä Dataset size after cleaning:", len(df))


# ---------------- BALANCE DATA ----------------
df_pos = df[df.label == "positive"]
df_neg = df[df.label == "negative"]
df_neu = df[df.label == "neutral"]

min_size = min(len(df_pos), len(df_neg), len(df_neu))

df_pos = resample(df_pos, n_samples=min_size, random_state=42)
df_neg = resample(df_neg, n_samples=min_size, random_state=42)
df_neu = resample(df_neu, n_samples=min_size, random_state=42)

df = pd.concat([df_pos, df_neg, df_neu])

print("\nüìä After Balancing:")
print(df["label"].value_counts())


# ---------------- WORD2VEC ----------------
print("\nüß† Training Word2Vec...")

sentences = [text.split() for text in df["text"]]

w2v_model = Word2Vec(
    sentences,
    vector_size=200,   # üî• improved
    window=7,
    min_count=2,
    workers=4,
    sg=1               # skip-gram
)

print("‚úÖ Word2Vec training completed")


# ---------------- TEXT ‚Üí VECTOR ----------------
def get_vector(text):
    words = text.split()
    vectors = [w2v_model.wv[w] for w in words if w in w2v_model.wv]

    if len(vectors) == 0:
        return np.zeros(200)

    return np.mean(vectors, axis=0) * (1 + len(vectors)/10)


# ---------------- FEATURES ----------------
X = np.array([get_vector(text) for text in df["text"]])
y = df["label"]


# ---------------- SPLIT ----------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)


# ---------------- MODEL ----------------
print("\nü§ñ Training Model...")

model = RandomForestClassifier(
    n_estimators=300,
    random_state=42
)

model.fit(X_train, y_train)

print("‚úÖ Model trained successfully")


# ---------------- EVALUATION ----------------
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"\n‚úÖ Accuracy: {accuracy:.2f}\n")
print("üìä Classification Report:")
print(classification_report(y_test, y_pred))


# ---------------- RULE BOOST ----------------
def rule_boost(text):
    # üî• direct phrase detection (VERY IMPORTANT)
    if "jobs are hard to find" in text:
        return "negative"
    if "unemployment" in text:
        return "negative"

    negative_words = [
        "price_increase", "job_loss",
        "negative_feeling", "failed", "bad", "high_cost"
    ]

    for word in negative_words:
        if word in text:
            return "negative"

    return None


# ---------------- REAL TEST ----------------
print("\nüîç Real-world Testing:")

test_sentences = [
    "People are frustrated with inflation",
    "Government is doing a great job",
    "The policy was announced yesterday",
    "Jobs are hard to find",
    "This decision is not good"
]

for text in test_sentences:

    # üî• FIRST apply rule on ORIGINAL text
    rule = rule_boost(text.lower())

    cleaned = clean_text(text)
    vec = get_vector(cleaned)

    if rule:
        pred = rule

        # -------- CONFIDENCE --------
        proba = [0.0, 0.0, 0.0]
        labels = ["negative", "neutral", "positive"]
        confidence = 100.0   # rule-based strong

    else:
        pred = model.predict([vec])[0]

        # -------- CONFIDENCE --------
        try:
            proba = model.predict_proba([vec])[0]
            confidence = round(max(proba) * 100, 2)
            labels = model.classes_
        except:
            proba = [0.33, 0.33, 0.34]
            labels = ["negative", "neutral", "positive"]
            confidence = 0

    print(f"{text} ‚Üí {pred} ({confidence}%)")


# ---------------- SAVE ----------------
os.makedirs("model", exist_ok=True)

with open("model/sentiment_model.pkl", "wb") as f:
    pickle.dump(model, f)

with open("model/w2v_model.pkl", "wb") as f:
    pickle.dump(w2v_model, f)

print("\nüíæ Model & Word2Vec saved successfully!")